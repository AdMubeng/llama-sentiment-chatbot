# -*- coding: utf-8 -*-
"""Bounty: Llama Chatbot with Sentiment Analysis Integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bfACnLebRszAFzGQL3ZLd-gU6SRF6b7E

### **Enhanced Llama 2 Chatbot with Sentiment Analysis**

In this project, we enhanced a Llama 2 chatbot by integrating sentiment analysis to adjust the chatbotâ€™s responses based on user sentiment. The chatbot first checks a QA dataset for predefined answers and uses the Llama 2 model to generate new responses when necessary. Sentiment analysis was implemented using a Hugging Face pipeline to detect positive or negative sentiments. The chatbot adjusts its tone and responses accordingly, offering more empathetic answers when negative sentiment is detected, or friendly and positive responses for users in a good mood. This project showcases the capability of AI to provide more emotionally intelligent interactions.

## **Step 1: Overview of Bounty**

The task is to modify the chatbot to detect the sentiment of user input and adjust its responses accordingly. For example, if the sentiment is negative, the chatbot might offer support or apologies, while positive sentiments might result in more friendly or enthusiastic responses.

# Step 2: Example Use Cases
Here are some examples of how sentiment analysis can enhance a chatbot's functionality:

**Customer Support Chatbot:** Apologizes when it detects user frustration or dissatisfaction.

**Mental Health Assistant:** Provides supportive and positive messages when a user is detected to be feeling down.

**Educational Tutor:** Adjusts its tone when a user appears frustrated, offering simpler explanations or encouraging messages.

## **Step 3: Boundaries for the Challenge**

**Mandatory Use of Sentiment Analysis:** Integrate a sentiment analysis model from Hugging Face and use it to adjust chatbot responses.

**Focus on Chatbot Functionality:** Ensure that sentiment analysis directly influences the responses.

**Tool Selection:** Use Google Colab, Python, and Hugging Face models, as required. Feel free to add other libraries that enhance the project.

## **Step 4: Procedure to Integrate Sentiment Analysis**

**Step 4.1: Setting Up the Environment**

You will need to set up your environment in Google Colab by installing the necessary packages.
"""

!pip install -q accelerate protobuf sentencepiece torch
!pip install git+https://github.com/huggingface/transformers huggingface_hub gradio
!pip install -q transformers pandas

"""**Step 4.2: Import Necessary Libraries**

Import all the necessary libraries, including Hugging Face models for Llama 2 and the sentiment analysis model.
"""

import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from huggingface_hub import login
import os
import torch
import gradio as gr

"""**Step 4.3: Login to Hugging Face**

You'll need to authenticate your Hugging Face account in order to access the pre-trained models.
"""

# Hugging Face Authentication
login(token="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")

"""**Step 4.4: Load the Llama 2 Chat Model**

Load the Llama 2 model and its tokenizer.
"""

# Initialize Llama 2 model and tokenizer
model_id = "NousResearch/Llama-2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.use_default_system_prompt = False

# Initialize the pipeline
llama_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.float16,
    device_map="auto",
    max_length=1024,
)

"""**Step 4.5: Add a Predefined QA Dataset**

Create or load an existing question-answer (QA) dataset.


"""

# Create or load QA dataset
csv_file = 'qa_dataset.csv'
if not os.path.exists(csv_file):
    qa_data = {
        'question': ["What is the capital of Nigeria?", "Who is the Best Cement Producer?", "How are you?"],
        'answer': ["The capital of Nigeria is Abuja.", "The Best Cement Producer is Dangote.", "I'm fine, Thank you."]
    }
    qa_df = pd.DataFrame(qa_data)
    qa_df.to_csv(csv_file, index=False)
else:
    qa_df = pd.read_csv(csv_file)

"""**Step 4.6: Load a Sentiment Analysis Model**

Add a sentiment analysis pipeline to your chatbot. We'll use a pre-trained sentiment analysis model from Hugging Face.
"""

# Initialize sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

"""**Step 4.7: Define the Main Chatbot Function**

Define a function that will process user questions. The function will first check the QA dataset, and if the answer is not found, it will generate a new one using Llama 2. The sentiment of the user input will influence the chatbot's tone.
"""

# Custom function to adjust response based on sentiment and context
def adjust_response_based_on_sentiment_and_context(response, sentiment, context):
    if context == "customer_support":
        if sentiment == "VERY_NEGATIVE":
            response += "\nI'm really sorry to hear you're experiencing issues. Let me escalate this and get you some help immediately."
        elif sentiment == "NEGATIVE":
            response += "\nI understand your frustration. Please let me assist you with your problem."
        elif sentiment == "NEUTRAL":
            response += "\nI see. Could you provide me with more details so I can better assist you?"
        elif sentiment == "POSITIVE":
            response += "\nI'm happy you're satisfied with our service. Let me know if there's anything else I can do!"
        elif sentiment == "VERY_POSITIVE":
            response += "\nThat's wonderful to hear! Thanks for your positive feedback!"
    else:  # Casual chat
        if sentiment == "VERY_NEGATIVE":
            response += "\nIt seems like you're having a tough time. I'm here to chat if you'd like."
        elif sentiment == "NEGATIVE":
            response += "\nI'm sorry if something is bothering you. Do you want to talk about it?"
        elif sentiment == "NEUTRAL":
            response += "\nI see. Feel free to ask me anything!"
        elif sentiment == "POSITIVE":
            response += "\nIt's great to hear you're doing well! Let's keep the conversation going."
        elif sentiment == "VERY_POSITIVE":
            response += "\nThat's awesome! You seem really excited!"

    return response

def classify_sentiment(user_input):
    sentiment_result = sentiment_pipeline(user_input)
    sentiment_label = sentiment_result[0]['label']
    score = sentiment_result[0]['score']

    # Map sentiment labels to more detailed categories
    if sentiment_label == "NEGATIVE":
        if score > 0.8:
            return "VERY_NEGATIVE"
        else:
            return "NEGATIVE"
    elif sentiment_label == "POSITIVE":
        if score > 0.8:
            return "VERY_POSITIVE"
        else:
            return "POSITIVE"
    else:
        return "NEUTRAL"

def answer_question(question, context="casual_chat"):
    global qa_df

    # Detect the sentiment of the user input
    sentiment = classify_sentiment(question)
    print(f"Detected Sentiment: {sentiment}")

    # Check if the question exists in the QA dataset
    answer = qa_df[qa_df['question'].str.lower() == question.lower()]['answer']

    if not answer.empty:
        response = f"Answer from QA dataset: {answer.iloc[0]}"
    else:
        # Generate a response using Llama 2
        response = llama_pipeline(question, max_length=150, do_sample=True)[0]['generated_text']

        # Add new question-answer pair to the dataset
        new_row = pd.DataFrame({'question': [question], 'answer': [response]})
        qa_df = pd.concat([qa_df, new_row], ignore_index=True)
        qa_df.to_csv(csv_file, index=False)
        response = f"Answer from Llama 2: {response} \n(New QA pair added to the dataset.)"

    # Adjust response based on sentiment and context
    response = adjust_response_based_on_sentiment_and_context(response, sentiment, context)

    return response

"""**Step 4.8: Test the Function**

Test the function with a few inputs to see how sentiment influences the chatbot's response
"""

print(answer_question("What is the capital of Nigeria?"))
print("\n")
print(answer_question("Why did my order take so long?"))
print("\n")
print(answer_question("How does the football used in 2010 look?"))

"""**Step 4.9: Create a Gradio Interface**

Finally, create a Gradio interface to allow real-time interaction with the chatbot.
"""

def gradio_chat_interface(question):
    return answer_question(question)

interface = gr.Interface(
    fn=gradio_chat_interface,
    inputs="text",
    outputs="text",
    title="Enhanced Llama 2 Chatbot with Sentiment Analysis",
    description="This chatbot adjusts its responses based on the sentiment detected in the user's input.",
)

# Launch the Gradio interface
interface.launch()

"""# **Full Code Example**

# Step 1: Install required libraries

# Step 2: Import libraries

# Step 3: Login to Hugging Face

# Step 4: Load Llama 2 model and tokenizer

# Step 5: Load or create QA dataset

# Step 6: Initialize sentiment analysis pipeline

# Step 7: Define main function to answer questions and adjust based on sentiment


"""