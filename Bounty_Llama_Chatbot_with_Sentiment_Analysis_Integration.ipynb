{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko4omRTfCqca"
      },
      "source": [
        "### **Enhanced Llama 2 Chatbot with Sentiment Analysis**\n",
        "\n",
        "In this project, we enhanced a Llama 2 chatbot by integrating sentiment analysis to adjust the chatbotâ€™s responses based on user sentiment. The chatbot first checks a QA dataset for predefined answers and uses the Llama 2 model to generate new responses when necessary. Sentiment analysis was implemented using a Hugging Face pipeline to detect positive or negative sentiments. The chatbot adjusts its tone and responses accordingly, offering more empathetic answers when negative sentiment is detected, or friendly and positive responses for users in a good mood. This project showcases the capability of AI to provide more emotionally intelligent interactions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3zoD1nr9Q1L"
      },
      "source": [
        "## **Step 1: Overview of Bounty**\n",
        "\n",
        "The task is to modify the chatbot to detect the sentiment of user input and adjust its responses accordingly. For example, if the sentiment is negative, the chatbot might offer support or apologies, while positive sentiments might result in more friendly or enthusiastic responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h48iYcpT9bwT"
      },
      "source": [
        "# Step 2: Example Use Cases\n",
        "Here are some examples of how sentiment analysis can enhance a chatbot's functionality:\n",
        "\n",
        "**Customer Support Chatbot:** Apologizes when it detects user frustration or dissatisfaction.\n",
        "\n",
        "**Mental Health Assistant:** Provides supportive and positive messages when a user is detected to be feeling down.\n",
        "\n",
        "**Educational Tutor:** Adjusts its tone when a user appears frustrated, offering simpler explanations or encouraging messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_X7Nsxq9xDV"
      },
      "source": [
        "## **Step 3: Boundaries for the Challenge**\n",
        "\n",
        "**Mandatory Use of Sentiment Analysis:** Integrate a sentiment analysis model from Hugging Face and use it to adjust chatbot responses.\n",
        "\n",
        "**Focus on Chatbot Functionality:** Ensure that sentiment analysis directly influences the responses.\n",
        "\n",
        "**Tool Selection:** Use Google Colab, Python, and Hugging Face models, as required. Feel free to add other libraries that enhance the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO91T3TW-EFC"
      },
      "source": [
        "## **Step 4: Procedure to Integrate Sentiment Analysis**\n",
        "\n",
        "**Step 4.1: Setting Up the Environment**\n",
        "\n",
        "You will need to set up your environment in Google Colab by installing the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MSJveDTr-x0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da334d4a-a355-4647-9c88-ea11e6634339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-922vqm84\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-922vqm84\n",
            "  Resolved https://github.com/huggingface/transformers to commit 5af7d41e49bbfc8319f462eb45253dcb3863dfb7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.5)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.6)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate protobuf sentencepiece torch\n",
        "!pip install git+https://github.com/huggingface/transformers huggingface_hub gradio\n",
        "!pip install -q transformers pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2STSQq6-qFf"
      },
      "source": [
        "**Step 4.2: Import Necessary Libraries**\n",
        "\n",
        "Import all the necessary libraries, including Hugging Face models for Llama 2 and the sentiment analysis model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1R4pNFZREtxB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import torch\n",
        "import gradio as gr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2G7w7yb_DOx"
      },
      "source": [
        "**Step 4.3: Login to Hugging Face**\n",
        "\n",
        "You'll need to authenticate your Hugging Face account in order to access the pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ynpLKtD__JZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4c45b9-0abd-4493-ed7d-6510a8a4f5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Hugging Face Authentication\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3aiulu_Urk"
      },
      "source": [
        "**Step 4.4: Load the Llama 2 Chat Model**\n",
        "\n",
        "Load the Llama 2 model and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rbJw-YuG_h1y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "59e57c7d1dc446b2b7c9247264f808a0",
            "14267901c9454d78aef5480ab09ce774",
            "3e5b68c4688e4330b0265958678df4c7",
            "ed63042466494feda54abb0f3e79feaa",
            "d51954bcd68240e8b1e3237580f41c71",
            "105344efc870452aa495ebc2f92732bd",
            "995323edb5784a5bb2fba17af92cc0a4",
            "5a151a9901e543ce9c9272e7c5b9367e",
            "4247a1b746ba4b4498cdb5f4a5111402",
            "370877d13a8342cc957d59894096c36d",
            "5ec3607233274f54bf60cca669934203"
          ]
        },
        "outputId": "64bc1239-8ea4-47f3-e5d5-d7932066884a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59e57c7d1dc446b2b7c9247264f808a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "# Initialize Llama 2 model and tokenizer\n",
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False\n",
        "\n",
        "# Initialize the pipeline\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-59zM0jAIo8"
      },
      "source": [
        "**Step 4.5: Add a Predefined QA Dataset**\n",
        "\n",
        "Create or load an existing question-answer (QA) dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZLAdO5pyALLW"
      },
      "outputs": [],
      "source": [
        "# Create or load QA dataset\n",
        "csv_file = 'qa_dataset.csv'\n",
        "if not os.path.exists(csv_file):\n",
        "    qa_data = {\n",
        "        'question': [\"What is the capital of Nigeria?\", \"Who is the Best Cement Producer?\", \"How are you?\"],\n",
        "        'answer': [\"The capital of Nigeria is Abuja.\", \"The Best Cement Producer is Dangote.\", \"I'm fine, Thank you.\"]\n",
        "    }\n",
        "    qa_df = pd.DataFrame(qa_data)\n",
        "    qa_df.to_csv(csv_file, index=False)\n",
        "else:\n",
        "    qa_df = pd.read_csv(csv_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyzmqTosAYmY"
      },
      "source": [
        "**Step 4.6: Load a Sentiment Analysis Model**\n",
        "\n",
        "Add a sentiment analysis pipeline to your chatbot. We'll use a pre-trained sentiment analysis model from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "owcqDYfFAb93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b257ae-c5ff-4102-b467-be4a2ea39dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ],
      "source": [
        "# Initialize sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkQ3SjFHAgDu"
      },
      "source": [
        "**Step 4.7: Define the Main Chatbot Function**\n",
        "\n",
        "Define a function that will process user questions. The function will first check the QA dataset, and if the answer is not found, it will generate a new one using Llama 2. The sentiment of the user input will influence the chatbot's tone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "g27CDoaCAuvc"
      },
      "outputs": [],
      "source": [
        " # Custom function to adjust response based on sentiment and context\n",
        "def adjust_response_based_on_sentiment_and_context(response, sentiment, context):\n",
        "    if context == \"customer_support\":\n",
        "        if sentiment == \"VERY_NEGATIVE\":\n",
        "            response += \"\\nI'm really sorry to hear you're experiencing issues. Let me escalate this and get you some help immediately.\"\n",
        "        elif sentiment == \"NEGATIVE\":\n",
        "            response += \"\\nI understand your frustration. Please let me assist you with your problem.\"\n",
        "        elif sentiment == \"NEUTRAL\":\n",
        "            response += \"\\nI see. Could you provide me with more details so I can better assist you?\"\n",
        "        elif sentiment == \"POSITIVE\":\n",
        "            response += \"\\nI'm happy you're satisfied with our service. Let me know if there's anything else I can do!\"\n",
        "        elif sentiment == \"VERY_POSITIVE\":\n",
        "            response += \"\\nThat's wonderful to hear! Thanks for your positive feedback!\"\n",
        "    else:  # Casual chat\n",
        "        if sentiment == \"VERY_NEGATIVE\":\n",
        "            response += \"\\nIt seems like you're having a tough time. I'm here to chat if you'd like.\"\n",
        "        elif sentiment == \"NEGATIVE\":\n",
        "            response += \"\\nI'm sorry if something is bothering you. Do you want to talk about it?\"\n",
        "        elif sentiment == \"NEUTRAL\":\n",
        "            response += \"\\nI see. Feel free to ask me anything!\"\n",
        "        elif sentiment == \"POSITIVE\":\n",
        "            response += \"\\nIt's great to hear you're doing well! Let's keep the conversation going.\"\n",
        "        elif sentiment == \"VERY_POSITIVE\":\n",
        "            response += \"\\nThat's awesome! You seem really excited!\"\n",
        "\n",
        "    return response\n",
        "\n",
        "def classify_sentiment(user_input):\n",
        "    sentiment_result = sentiment_pipeline(user_input)\n",
        "    sentiment_label = sentiment_result[0]['label']\n",
        "    score = sentiment_result[0]['score']\n",
        "\n",
        "    # Map sentiment labels to more detailed categories\n",
        "    if sentiment_label == \"NEGATIVE\":\n",
        "        if score > 0.8:\n",
        "            return \"VERY_NEGATIVE\"\n",
        "        else:\n",
        "            return \"NEGATIVE\"\n",
        "    elif sentiment_label == \"POSITIVE\":\n",
        "        if score > 0.8:\n",
        "            return \"VERY_POSITIVE\"\n",
        "        else:\n",
        "            return \"POSITIVE\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "def answer_question(question, context=\"casual_chat\"):\n",
        "    global qa_df\n",
        "\n",
        "    # Detect the sentiment of the user input\n",
        "    sentiment = classify_sentiment(question)\n",
        "    print(f\"Detected Sentiment: {sentiment}\")\n",
        "\n",
        "    # Check if the question exists in the QA dataset\n",
        "    answer = qa_df[qa_df['question'].str.lower() == question.lower()]['answer']\n",
        "\n",
        "    if not answer.empty:\n",
        "        response = f\"Answer from QA dataset: {answer.iloc[0]}\"\n",
        "    else:\n",
        "        # Generate a response using Llama 2\n",
        "        response = llama_pipeline(question, max_length=150, do_sample=True)[0]['generated_text']\n",
        "\n",
        "        # Add new question-answer pair to the dataset\n",
        "        new_row = pd.DataFrame({'question': [question], 'answer': [response]})\n",
        "        qa_df = pd.concat([qa_df, new_row], ignore_index=True)\n",
        "        qa_df.to_csv(csv_file, index=False)\n",
        "        response = f\"Answer from Llama 2: {response} \\n(New QA pair added to the dataset.)\"\n",
        "\n",
        "    # Adjust response based on sentiment and context\n",
        "    response = adjust_response_based_on_sentiment_and_context(response, sentiment, context)\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kNjkynIA_j8"
      },
      "source": [
        "**Step 4.8: Test the Function**\n",
        "\n",
        "Test the function with a few inputs to see how sentiment influences the chatbot's response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_FFwte6XBCTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c3e6e9-d129-4b17-ffd0-92abf36cb55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Sentiment: VERY_NEGATIVE\n",
            "Answer from QA dataset: The capital of Nigeria is Abuja.\n",
            "It seems like you're having a tough time. I'm here to chat if you'd like.\n",
            "\n",
            "\n",
            "Detected Sentiment: VERY_NEGATIVE\n",
            "Answer from QA dataset: Why did my order take so long?\n",
            "I understand that sometimes orders can take longer than expected to arrive due to various reasons such as high demand, shipping delays, or unexpected issues with the fulfillment process. However, I would appreciate it if you could provide me with more detailed information about the status of my order and an estimated delivery date.\n",
            "\n",
            "Please let me know if there are any issues with my order or if there is anything else I can do to expedite the delivery process. I appreciate your attention to this matter and look forward to hearing back from you soon.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "It seems like you're having a tough time. I'm here to chat if you'd like.\n",
            "\n",
            "\n",
            "Detected Sentiment: VERY_NEGATIVE\n",
            "Answer from Llama 2: How does the football used in 2010 look?\n",
            "\n",
            "The football used in the 2010 FIFA World Cup was the Adidas Jabulani. It was designed to be a low-weight, high-velocity ball with a unique design that featured a mixture of panels and a textured surface. The ball was intended to move unpredictably in flight, making it more difficult for goalkeepers to predict its trajectory.\n",
            "\n",
            "Here are some key features of the Adidas Jabulani:\n",
            "\n",
            "* Weight: 410g (14.1 oz)\n",
            "* Size: 57-63 cm (22.4-24 \n",
            "(New QA pair added to the dataset.)\n",
            "It seems like you're having a tough time. I'm here to chat if you'd like.\n"
          ]
        }
      ],
      "source": [
        "print(answer_question(\"What is the capital of Nigeria?\"))\n",
        "print(\"\\n\")\n",
        "print(answer_question(\"Why did my order take so long?\"))\n",
        "print(\"\\n\")\n",
        "print(answer_question(\"How does the football used in 2010 look?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKM0SUkxBFf5"
      },
      "source": [
        "**Step 4.9: Create a Gradio Interface**\n",
        "\n",
        "Finally, create a Gradio interface to allow real-time interaction with the chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Y4SxWpw_BMio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "035dd042-972c-4bde-9fa1-9a3f08b71bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://db561674cf9af9d9c1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://db561674cf9af9d9c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "def gradio_chat_interface(question):\n",
        "    return answer_question(question)\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_chat_interface,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Enhanced Llama 2 Chatbot with Sentiment Analysis\",\n",
        "    description=\"This chatbot adjusts its responses based on the sentiment detected in the user's input.\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrtP2VDHBWT3"
      },
      "source": [
        "# **Full Code Example**\n",
        "\n",
        "# Step 1: Install required libraries\n",
        "\n",
        "# Step 2: Import libraries\n",
        "\n",
        "# Step 3: Login to Hugging Face\n",
        "\n",
        "# Step 4: Load Llama 2 model and tokenizer\n",
        "\n",
        "# Step 5: Load or create QA dataset\n",
        "\n",
        "# Step 6: Initialize sentiment analysis pipeline\n",
        "\n",
        "# Step 7: Define main function to answer questions and adjust based on sentiment\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59e57c7d1dc446b2b7c9247264f808a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14267901c9454d78aef5480ab09ce774",
              "IPY_MODEL_3e5b68c4688e4330b0265958678df4c7",
              "IPY_MODEL_ed63042466494feda54abb0f3e79feaa"
            ],
            "layout": "IPY_MODEL_d51954bcd68240e8b1e3237580f41c71"
          }
        },
        "14267901c9454d78aef5480ab09ce774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_105344efc870452aa495ebc2f92732bd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_995323edb5784a5bb2fba17af92cc0a4",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "3e5b68c4688e4330b0265958678df4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a151a9901e543ce9c9272e7c5b9367e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4247a1b746ba4b4498cdb5f4a5111402",
            "value": 2
          }
        },
        "ed63042466494feda54abb0f3e79feaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_370877d13a8342cc957d59894096c36d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5ec3607233274f54bf60cca669934203",
            "value": "â€‡2/2â€‡[00:45&lt;00:00,â€‡20.41s/it]"
          }
        },
        "d51954bcd68240e8b1e3237580f41c71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105344efc870452aa495ebc2f92732bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "995323edb5784a5bb2fba17af92cc0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a151a9901e543ce9c9272e7c5b9367e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4247a1b746ba4b4498cdb5f4a5111402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "370877d13a8342cc957d59894096c36d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ec3607233274f54bf60cca669934203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}